{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "-6wN2Ej90oPo",
        "outputId": "7d137139-41d6-4159-9f73-9aaef6e5be33"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2994569370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Montar Drive + caminhos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mBASE_DIR\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/tech-challenge-3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Montar Drive + caminhos\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "BASE_DIR  = \"/content/drive/MyDrive/tech-challenge-3\"\n",
        "TRAIN_PATH = f\"{BASE_DIR}/trn.json.gz\"\n",
        "TEST_PATH  = f\"{BASE_DIR}/tst.json.gz\"\n",
        "print(\"Train:\", TRAIN_PATH)\n",
        "print(\"Test :\", TEST_PATH)\n",
        "\n",
        "# Ambiente\n",
        "import os, random, numpy as np, torch\n",
        "from transformers import set_seed\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n",
        "\n",
        "# (Re)instalar só se faltar o pacote na sessão\n",
        "# %pip -q install \"transformers==4.41.2\" \"datasets==2.20.0\" \"accelerate==0.32.0\" \"peft==0.11.1\" \"sentencepiece==0.1.99\" \"rouge-score==0.1.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okZrmEQX0rnl"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def clean(x): return (x or \"\").strip()\n",
        "\n",
        "raw = load_dataset(\"json\", data_files={\"train\": TRAIN_PATH, \"test\": TEST_PATH})\n",
        "\n",
        "def build_fields_en(example):\n",
        "    title   = clean(example.get(\"title\",\"\"))\n",
        "    content = clean(example.get(\"content\",\"\"))\n",
        "    example[\"prompt\"] = (\n",
        "        \"Task: Generate the product CONTENT from its TITLE.\\n\"\n",
        "        f\"TITLE: {title}\\n\"\n",
        "        \"CONTENT:\"\n",
        "    )\n",
        "    example[\"target\"] = content\n",
        "    return example\n",
        "\n",
        "raw = raw.map(build_fields_en)\n",
        "raw = raw.filter(lambda ex: len(clean(ex[\"target\"])) > 0)\n",
        "\n",
        "# Amostras para treino/val\n",
        "train_sample_size = 120_000\n",
        "test_sample_size  = 4_000\n",
        "train_ds = raw[\"train\"].shuffle(seed=42).select(range(min(train_sample_size, len(raw[\"train\"]))))\n",
        "eval_ds  = raw[\"test\"].shuffle(seed=42).select(range(min(test_sample_size,  len(raw[\"test\"]))))\n",
        "len(train_ds), len(eval_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_bsJL0w0tpJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
        "\n",
        "BASE_MODEL = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "MAX_INPUT, MAX_TARGET = 256, 256\n",
        "def tokenize(batch):\n",
        "    x = tokenizer(batch[\"prompt\"], max_length=MAX_INPUT, truncation=True)\n",
        "    y = tokenizer(text_target=batch[\"target\"], max_length=MAX_TARGET, truncation=True)\n",
        "    x[\"labels\"] = y[\"input_ids\"]\n",
        "    return x\n",
        "\n",
        "tokenized_train = train_ds.map(tokenize, batched=True, remove_columns=train_ds.column_names)\n",
        "tokenized_eval  = eval_ds.map(tokenize,  batched=True, remove_columns=eval_ds.column_names)\n",
        "collator = DataCollatorForSeq2Seq(tokenizer)\n",
        "print(tokenized_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo base (sem treino)\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL).to(device)\n",
        "base_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_base(title):\n",
        "    prompt = (\n",
        "        \"Task: Generate the product CONTENT from its TITLE.\\n\"\n",
        "        f\"TITLE: {title}\\n\"\n",
        "        \"CONTENT:\"\n",
        "    )\n",
        "    x = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    y = base_model.generate(**x, max_new_tokens=128, num_beams=4)\n",
        "    return tokenizer.decode(y[0], skip_special_tokens=True).strip()\n",
        "\n",
        "# Teste rápido ANTES do treino\n",
        "print(\"ANTES do treino:\", infer_base(\"Apple iPhone 15 Pro Max 256GB, Natural Titanium\"))\n"
      ],
      "metadata": {
        "id": "uW1Ff-c3sY7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjSzvEFF0uHs"
      },
      "outputs": [],
      "source": [
        "# 4) Treino RÁPIDO com LoRA (compatível com versões novas/antigas do Transformers)\n",
        "\n",
        "import os, gc, torch\n",
        "from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Limpar GPU\n",
        "for obj in [\"trainer\", \"model\"]:\n",
        "    try: del globals()[obj]\n",
        "    except: pass\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Amostra rápida (ajuste se quiser mais)\n",
        "FAST_TRAIN_SIZE = 40_000\n",
        "small_train = tokenized_train.select(range(min(FAST_TRAIN_SIZE, len(tokenized_train))))\n",
        "print(\"Train rápido:\", len(small_train))\n",
        "\n",
        "# Modelo + LoRA\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)\n",
        "model.config.use_cache = False\n",
        "model = model.to(device)\n",
        "\n",
        "lora_cfg = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, r=4, lora_alpha=8, lora_dropout=0.05, target_modules=[\"q\",\"v\"])\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "# Args comuns (versão-agnóstico)\n",
        "common_kwargs = dict(\n",
        "    output_dir=f\"{BASE_DIR}/out_fast_lora_en\",\n",
        "    max_steps=2000,\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Tenta usar a API nova; se não existir, usa a antiga\n",
        "try:\n",
        "    args = TrainingArguments(**common_kwargs, eval_strategy=\"no\")            # Transformers >= 4.46\n",
        "except TypeError:\n",
        "    args = TrainingArguments(**common_kwargs, evaluation_strategy=\"no\")      # Transformers <= 4.45\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=small_train,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "train_result = trainer.train()\n",
        "print(train_result)\n",
        "\n",
        "SAVE_DIR = f\"{BASE_DIR}/flan_t5_small_lora_fast_en\"\n",
        "trainer.save_model(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "print(\"Salvo em:\", SAVE_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfbT5FNO0vpv"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "base_ft = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL).to(device)\n",
        "model_ft = PeftModel.from_pretrained(base_ft, SAVE_DIR).to(device)\n",
        "model_ft.eval()\n",
        "\n",
        "def make_prompt_en(title, question=None):\n",
        "    return (\n",
        "        \"Task: Generate the product CONTENT from its TITLE.\\n\"\n",
        "        f\"TITLE: {title}\\n\"\n",
        "        \"CONTENT:\"\n",
        "    )\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer(title, question=\"What is the content of this product?\"):\n",
        "    x = tokenizer(make_prompt_en(title, question), return_tensors=\"pt\").to(device)\n",
        "    y = model_ft.generate(\n",
        "      **x,\n",
        "      max_new_tokens=496,\n",
        "      min_new_tokens=128,         # garante saída mínima\n",
        "      do_sample=False,\n",
        "      num_beams=4,\n",
        "      length_penalty=1.0,\n",
        "      no_repeat_ngram_size=3,\n",
        "    )\n",
        "    return tokenizer.decode(y[0], skip_special_tokens=True).strip()\n",
        "\n",
        "# Smoke test\n",
        "print(infer(\"Apple iPhone 15 Pro Max 256GB, Natural Titanium\")[:200])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ti3trK40wxu"
      },
      "outputs": [],
      "source": [
        "# ROUGE-L: instala o pacote se faltar e avalia\n",
        "import sys, subprocess\n",
        "\n",
        "# 1) Garantir a dependência\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"rouge-score\"])\n",
        "    from rouge_score import rouge_scorer\n",
        "\n",
        "# 2) Imports restantes\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# 3) Sanidade rápida (espera que 'infer' e 'eval_ds' já existam das células anteriores)\n",
        "if 'infer' not in globals():\n",
        "    raise RuntimeError(\"Função 'infer' não encontrada. Execute a célula de INFERÊNCIA antes desta.\")\n",
        "if 'eval_ds' not in globals():\n",
        "    raise RuntimeError(\"Dataset 'eval_ds' não encontrado. Execute a célula de DADOS antes desta.\")\n",
        "\n",
        "# 4) Avaliação\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "def eval_rouge(sample=10, show=10):\n",
        "    subset = eval_ds.select(range(min(sample, len(eval_ds))))\n",
        "    f1s = []\n",
        "    for i, ex in enumerate(tqdm(subset)):\n",
        "        pred = infer(ex[\"title\"])\n",
        "        ref  = (ex[\"content\"] or \"\").strip()\n",
        "        f1s.append(scorer.score(ref, pred)[\"rougeL\"].fmeasure)\n",
        "        if i < show:\n",
        "            print(\"\\n\" + \"—\"*40)\n",
        "            print(\"[TITLE]\", ex[\"title\"][:160])\n",
        "            print(\"[PRED ]\", pred[:300])\n",
        "            print(\"[GOLD ]\", ref[:300])\n",
        "    return {\"rougeL_f1_mean\": float(np.mean(f1s)), \"n\": len(subset)}\n",
        "\n",
        "eval_rouge(10, show=10)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}